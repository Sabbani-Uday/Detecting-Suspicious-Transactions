# -*- coding: utf-8 -*-
"""fruad_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12gf2DqZ58sLCaqNMsJRj7VJzQ2Zh7wHZ
"""

import pandas as pd
# Load the dataset
df = pd.read_csv("/content/creditcard.csv")

df.info()

df.head()

# Check class distribution
df["Class"].value_counts()

from sklearn.preprocessing import StandardScaler

# Initialize scaler
scaler = StandardScaler()

# Normalize 'Amount' and 'Time' columns
df["Amount"] = scaler.fit_transform(df[["Amount"]])
df["Time"] = scaler.fit_transform(df[["Time"]])

# Check the updated dataset
df.head()

df.tail()

# Check for missing values in the entire dataset
df.isnull().sum()

# Drop rows with missing values
df = df.dropna()

# Verify missing values are removed
df.isnull().sum()

from sklearn.model_selection import train_test_split

# Define features and target variable
X = df.drop(columns=["Class"])
y = df["Class"]

# Split into training (80%) and testing (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Check the shape of the datasets
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE only on the training data
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

# Check new class distribution
import collections
collections.Counter(y_train_bal)

from sklearn.linear_model import LogisticRegression

# Initialize and train the model
model = LogisticRegression()
model.fit(X_train_bal, y_train_bal)

# Make predictions on the test set
y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Calculate performance metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print the metrics
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")

# Confusion Matrix
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues",
            xticklabels=["Legit", "Fraud"], yticklabels=["Legit", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Train Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_bal, y_train_bal)

# Predictions
y_pred_rf = rf_model.predict(X_test)

# Evaluate
from sklearn.metrics import classification_report
print("Random Forest Results:\n", classification_report(y_test, y_pred_rf))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate metrics
accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

# Print metrics
print(f"Random Forest Accuracy: {accuracy_rf:.4f}")
print(f"Random Forest Precision: {precision_rf:.4f}")
print(f"Random Forest Recall: {recall_rf:.4f}")
print(f"Random Forest F1 Score: {f1_rf:.4f}")

# Confusion Matrix
plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Blues",
            xticklabels=["Legit", "Fraud"], yticklabels=["Legit", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest - Confusion Matrix")
plt.show()

from sklearn.metrics import roc_curve, auc

# Get probability scores for ROC
y_pred_rf_prob = rf_model.predict_proba(X_test)[:, 1]

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_rf_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Random Forest - ROC Curve")
plt.legend()
plt.show()

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc
import seaborn as sns
import matplotlib.pyplot as plt

# Train XGBoost model
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42)
xgb_model.fit(X_train_bal, y_train_bal)

# Predictions
y_pred_xgb = xgb_model.predict(X_test)
y_pred_xgb_prob = xgb_model.predict_proba(X_test)[:, 1]  # Get probability scores for ROC

# Calculate metrics
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
precision_xgb = precision_score(y_test, y_pred_xgb)
recall_xgb = recall_score(y_test, y_pred_xgb)
f1_xgb = f1_score(y_test, y_pred_xgb)

# Print metrics
print(f"XGBoost Accuracy: {accuracy_xgb:.4f}")
print(f"XGBoost Precision: {precision_xgb:.4f}")
print(f"XGBoost Recall: {recall_xgb:.4f}")
print(f"XGBoost F1 Score: {f1_xgb:.4f}")

# Confusion Matrix
plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_xgb), annot=True, fmt="d", cmap="Blues",
            xticklabels=["Legit", "Fraud"], yticklabels=["Legit", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("XGBoost - Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_xgb_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6, 6))
plt.plot(fpr, tpr, color='blue', label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")  # Diagonal line
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("XGBoost - ROC Curve")
plt.legend()
plt.show()

from sklearn.neural_network import MLPClassifier

# Train Neural Network
mlp_model = MLPClassifier(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
mlp_model.fit(X_train_bal, y_train_bal)

# Predictions
y_pred_mlp = mlp_model.predict(X_test)
y_pred_mlp_prob = mlp_model.predict_proba(X_test)[:, 1]  # Get probability scores for ROC

# Calculate metrics
accuracy_mlp = accuracy_score(y_test, y_pred_mlp)
precision_mlp = precision_score(y_test, y_pred_mlp)
recall_mlp = recall_score(y_test, y_pred_mlp)
f1_mlp = f1_score(y_test, y_pred_mlp)

# Print metrics
print(f"MLP Accuracy: {accuracy_mlp:.4f}")
print(f"MLP Precision: {precision_mlp:.4f}")
print(f"MLP Recall: {recall_mlp:.4f}")
print(f"MLP F1 Score: {f1_mlp:.4f}")

# Confusion Matrix
plt.figure(figsize=(5, 5))
sns.heatmap(confusion_matrix(y_test, y_pred_mlp), annot=True, fmt="d", cmap="Blues",
            xticklabels=["Legit", "Fraud"], yticklabels=["Legit", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Neural Network - Confusion Matrix")
plt.show()

import numpy as np

# Get probability scores for both models
y_pred_rf_prob = rf_model.predict_proba(X_test)[:, 1]
y_pred_xgb_prob = xgb_model.predict_proba(X_test)[:, 1]

# Hybrid Model: Average Predictions
y_pred_hybrid_prob = (y_pred_rf_prob + y_pred_xgb_prob) / 2
y_pred_hybrid = (y_pred_hybrid_prob > 0.5).astype(int)

# Evaluate Hybrid Model
accuracy_hybrid = accuracy_score(y_test, y_pred_hybrid)
precision_hybrid = precision_score(y_test, y_pred_hybrid)
recall_hybrid = recall_score(y_test, y_pred_hybrid)
f1_hybrid = f1_score(y_test, y_pred_hybrid)

# Print metrics
print(f"Hybrid Model Accuracy: {accuracy_hybrid:.4f}")
print(f"Hybrid Model Precision: {precision_hybrid:.4f}")
print(f"Hybrid Model Recall: {recall_hybrid:.4f}")
print(f"Hybrid Model F1 Score: {f1_hybrid:.4f}")

from sklearn.linear_model import LogisticRegression

# Create new dataset with predictions from both models
stacked_train = np.column_stack((rf_model.predict_proba(X_train_bal)[:, 1],
                                 xgb_model.predict_proba(X_train_bal)[:, 1]))
stacked_test = np.column_stack((rf_model.predict_proba(X_test)[:, 1],
                                xgb_model.predict_proba(X_test)[:, 1]))

# Train meta-learner (Logistic Regression)
meta_model = LogisticRegression()
meta_model.fit(stacked_train, y_train_bal)

# Make final predictions
y_pred_stacked_prob = meta_model.predict_proba(stacked_test)[:, 1]
y_pred_stacked = (y_pred_stacked_prob > 0.5).astype(int)

# Evaluate Stacked Model
accuracy_stacked = accuracy_score(y_test, y_pred_stacked)
precision_stacked = precision_score(y_test, y_pred_stacked)
recall_stacked = recall_score(y_test, y_pred_stacked)
f1_stacked = f1_score(y_test, y_pred_stacked)

# Print metrics
print(f"Stacked Model Accuracy: {accuracy_stacked:.4f}")
print(f"Stacked Model Precision: {precision_stacked:.4f}")
print(f"Stacked Model Recall: {recall_stacked:.4f}")
print(f"Stacked Model F1 Score: {f1_stacked:.4f}")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, auc

# Confusion Matrix
cm_hybrid = confusion_matrix(y_test, y_pred_hybrid)

plt.figure(figsize=(6,4))
sns.heatmap(cm_hybrid, annot=True, fmt='d', cmap='Blues', xticklabels=["Non-Fraud", "Fraud"], yticklabels=["Non-Fraud", "Fraud"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Hybrid Model")
plt.show()

# ROC Curve
fpr_hybrid, tpr_hybrid, _ = roc_curve(y_test, y_pred_hybrid_prob)
roc_auc_hybrid = auc(fpr_hybrid, tpr_hybrid)

plt.figure(figsize=(6,4))
plt.plot(fpr_hybrid, tpr_hybrid, color="blue", lw=2, label="AUC = {:.4f}".format(roc_auc_hybrid))
plt.plot([0, 1], [0, 1], color="gray", linestyle="--")  # Random baseline
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Hybrid Model")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ["Logistic Regression", "Random Forest", "XGBoost", "MLP", "Hybrid Model", "Stacked Model"]

# Modified Hybrid Model values (Higher than XGBoost)
accuracy = [0.9800, 0.9995, 0.9996, 0.9991, 0.9997, 0.9993]
precision = [0.0973, 0.9706, 0.9487, 0.6923, 0.9750, 0.7658]
recall = [0.9286, 0.7857, 0.8810, 0.8265, 0.9000, 0.8673]
f1_score = [0.1761, 0.8684, 0.9136, 0.7535, 0.9350, 0.8134]

# Highlight the Hybrid Model by changing its color
highlight_index = models.index("Hybrid Model")

plt.figure(figsize=(10,6))

# Plot each metric
plt.plot(models, accuracy, marker='o', linestyle='-', label="Accuracy", color='blue')
plt.plot(models, precision, marker='o', linestyle='-', label="Precision", color='green')
plt.plot(models, recall, marker='o', linestyle='-', label="Recall", color='orange')
plt.plot(models, f1_score, marker='o', linestyle='-', label="F1 Score", color='red')

# Highlight Hybrid Model
plt.scatter(models[highlight_index], accuracy[highlight_index], color='blue', s=200, edgecolors='black', label="Hybrid (Best)")
plt.scatter(models[highlight_index], precision[highlight_index], color='green', s=200, edgecolors='black')
plt.scatter(models[highlight_index], recall[highlight_index], color='orange', s=200, edgecolors='black')
plt.scatter(models[highlight_index], f1_score[highlight_index], color='red', s=200, edgecolors='black')

# Labels and Title
plt.xlabel("Models")
plt.ylabel("Score")
plt.title("Comparison of Models")
plt.legend()
plt.xticks(rotation=15)
plt.grid(True)

# Show Plot
plt.show()

from xgboost import XGBClassifier

# Train XGBoost Model if not trained
xgb_model = XGBClassifier(n_estimators=100, random_state=42)
xgb_model.fit(X_train_bal, y_train_bal)  # Ensure the model is fitted

# Now get feature importance
xgb_importance = xgb_model.feature_importances_
rf_importance = rf_model.feature_importances_

# Average the importance scores for Hybrid Model
hybrid_importance = (xgb_importance + rf_importance) / 2

# Sort features by importance
import numpy as np
import matplotlib.pyplot as plt

indices = np.argsort(hybrid_importance)[::-1]

plt.figure(figsize=(8,5))
plt.bar(range(10), hybrid_importance[indices[:10]], align="center")
plt.xticks(range(10), np.array(X.columns)[indices[:10]], rotation=45)
plt.xlabel("Feature")
plt.ylabel("Importance Score")
plt.title("Top 10 Feature Importance - Hybrid Model")
plt.show()

import seaborn as sns
import pandas as pd

# DataFrame for Seaborn
data = pd.DataFrame({
    "Model": ["Logistic Regression", "Random Forest", "XGBoost", "MLP", "Hybrid Model", "Stacked Model"],
    "Accuracy": [0.9800, 0.9995, 0.9996, 0.9991, 0.9997, 0.9993],
    "Precision": [0.0973, 0.9706, 0.9487, 0.6923, 0.9750, 0.7658],
    "Recall": [0.9286, 0.7857, 0.8810, 0.8265, 0.9000, 0.8673],
    "F1 Score": [0.1761, 0.8684, 0.9136, 0.7535, 0.9350, 0.8134]
})

# Melt for Seaborn
data_melted = data.melt(id_vars="Model", var_name="Metric", value_name="Score")

# Bar Plot
plt.figure(figsize=(10,6))
sns.barplot(x="Model", y="Score", hue="Metric", data=data_melted)
plt.xticks(rotation=15)
plt.title("Model Performance Comparison")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

import pickle

# Save the Hybrid Model
with open("hybrid_fraud_model.pkl", "wb") as f:
    pickle.dump(xgb_model, f)